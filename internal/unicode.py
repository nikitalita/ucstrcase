#!/usr/bin/env python
#
# Copyright 2011-2018 The Rust Project Developers. See the COPYRIGHT
# file at the top-level directory of this distribution and at
# http://rust-lang.org/COPYRIGHT.
#
# Licensed under the Apache License, Version 2.0 <LICENSE-APACHE or
# http://www.apache.org/licenses/LICENSE-2.0> or the MIT license
# <LICENSE-MIT or http://opensource.org/licenses/MIT>, at your
# option. This file may not be copied, modified, or distributed
# except according to those terms.

# This script uses the following Unicode tables:
# - DerivedNormalizationProps.txt
# - NormalizationTest.txt
# - UnicodeData.txt
# - StandardizedVariants.txt
#
# Since this should not require frequent updates, we just store this
# out-of-line and check the tables.rs and normalization_tests.rs files into git.
import collections
import os
import subprocess
import sys
import urllib.request

# from itertools import batched
# instead if importing batched (which requires Python 3.12), just make our own
def batched(iterable, n):
    batch = []
    for item in iterable:
        batch.append(item)
        if len(batch) == n:
            yield batch
            batch = []
    if batch:
        yield batch

def get_unicode_version():
    # check the args for --unicode <version>
    if len(sys.argv) > 1:
        for i, arg in enumerate(sys.argv):
            if arg == "--unicode" and i + 1 < len(sys.argv):
                return sys.argv[i + 1]
    return "15.1.0"

UNICODE_VERSION = get_unicode_version()
UCD_URL = "https://www.unicode.org/Public/%s/ucd/" % UNICODE_VERSION

PREAMBLE = """
// NOTE: The following code was generated by "internal/unicode.py", do not edit directly
//
// Generated from Unicode Version %s
//
// clang-format off
""" % UNICODE_VERSION

HEADER_CONTENTS = """
#pragma once
#include <stdint.h>
#include <stddef.h>
#include <stdbool.h>
typedef struct{
    uint32_t key;
    uint16_t value;
} CompositionTableKV;

typedef struct {
    uint16_t a;
    uint16_t b;
} u16Pair;

typedef struct {
    uint32_t code;
    u16Pair decomp;
} DecomposedKV;

#ifdef __cplusplus
extern "C" {
#endif
uint32_t composition_table_astral(uint32_t ch, uint32_t c2);
const uint32_t stream_safe_leading_nonstarters(uint32_t c);
#ifdef __cplusplus
}
#endif
"""

NormalizationTest = collections.namedtuple(
    "NormalizationTest",
    ["test_comment", "test_name", "source", "nfc", "nfd", "nfkc", "nfkd"],
)

# Mapping taken from Table 12 from:
# http://www.unicode.org/reports/tr44/#General_Category_Values
expanded_categories = {
    'Lu': ['LC', 'L'], 'Ll': ['LC', 'L'], 'Lt': ['LC', 'L'],
    'Lm': ['L'], 'Lo': ['L'],
    'Mn': ['M'], 'Mc': ['M'], 'Me': ['M'],
    'Nd': ['N'], 'Nl': ['N'], 'No': ['No'],
    'Pc': ['P'], 'Pd': ['P'], 'Ps': ['P'], 'Pe': ['P'],
    'Pi': ['P'], 'Pf': ['P'], 'Po': ['P'],
    'Sm': ['S'], 'Sc': ['S'], 'Sk': ['S'], 'So': ['S'],
    'Zs': ['Z'], 'Zl': ['Z'], 'Zp': ['Z'],
    'Cc': ['C'], 'Cf': ['C'], 'Cs': ['C'], 'Co': ['C'], 'Cn': ['C'],
}

# Constants from Unicode 9.0.0 Section 3.12 Conjoining Jamo Behavior
# http://www.unicode.org/versions/Unicode9.0.0/ch03.pdf#M9.32468.Heading.310.Combining.Jamo.Behavior
S_BASE, L_COUNT, V_COUNT, T_COUNT = 0xAC00, 19, 21, 28
S_COUNT = L_COUNT * V_COUNT * T_COUNT

class UnicodeData(object):
    def __init__(self):
        self._load_unicode_data()
        self.norm_props = self._load_norm_props()
        self.norm_tests = self._load_norm_tests()

        self.canon_comp = self._compute_canonical_comp()
        self.canon_fully_decomp, self.compat_fully_decomp = self._compute_fully_decomposed()

        self.cjk_compat_variants_fully_decomp = {}
        self._load_cjk_compat_ideograph_variants()

        def stats(name, table):
            count = sum(len(v) for v in table.values())
            print("%s: %d chars => %d decomposed chars" % (name, len(table), count))

        print("Decomposition table stats:")
        stats("Canonical decomp", self.canon_decomp)
        stats("Compatible decomp", self.compat_decomp)
        stats("Canonical fully decomp", self.canon_fully_decomp)
        stats("Compatible fully decomp", self.compat_fully_decomp)
        stats("CJK Compat Variants fully decomp", self.cjk_compat_variants_fully_decomp)

        self.ss_leading, self.ss_trailing = self._compute_stream_safe_tables()

    def _fetch(self, filename):
        resp = urllib.request.urlopen(UCD_URL + filename)
        return resp.read().decode('utf-8')

    def _load_unicode_data(self):
        self.name_to_char_int = {}
        self.combining_classes = {}
        self.compat_decomp = {}
        self.canon_decomp = {}
        self.general_category_mark = []
        self.general_category_public_assigned = []

        assigned_start = 0;
        prev_char_int = -1;
        prev_name = "";

        for line in self._fetch("UnicodeData.txt").splitlines():
            # See ftp://ftp.unicode.org/Public/3.0-Update/UnicodeData-3.0.0.html
            pieces = line.split(';')
            assert len(pieces) == 15
            char, name, category, cc, decomp = pieces[0], pieces[1], pieces[2], pieces[3], pieces[5]
            char_int = int(char, 16)

            name = pieces[1].strip()
            self.name_to_char_int[name] = char_int

            if cc != '0':
                self.combining_classes[char_int] = cc

            if decomp.startswith('<'):
                self.compat_decomp[char_int] = [int(c, 16) for c in decomp.split()[1:]]
            elif decomp != '':
                self.canon_decomp[char_int] = [int(c, 16) for c in decomp.split()]

            if category == 'M' or 'M' in expanded_categories.get(category, []):
                self.general_category_mark.append(char_int)

            assert category != 'Cn', "Unexpected: Unassigned codepoint in UnicodeData.txt"
            if category not in ['Co', 'Cs']:
                if char_int != prev_char_int + 1 and not is_first_and_last(prev_name, name):
                    self.general_category_public_assigned.append((assigned_start, prev_char_int))
                    assigned_start = char_int
                prev_char_int = char_int
                prev_name = name;

        self.general_category_public_assigned.append((assigned_start, prev_char_int))

    def _load_cjk_compat_ideograph_variants(self):
        for line in self._fetch("StandardizedVariants.txt").splitlines():
            strip_comments = line.split('#', 1)[0].strip()
            if not strip_comments:
                continue

            variation_sequence, description, differences = strip_comments.split(';')
            description = description.strip()

            # Don't use variations that only apply in particular shaping environments.
            if differences:
                continue

            # Look for entries where the description field is a codepoint name.
            if description not in self.name_to_char_int:
                continue

            # Only consider the CJK Compatibility Ideographs.
            if not description.startswith('CJK COMPATIBILITY IDEOGRAPH-'):
                continue

            char_int = self.name_to_char_int[description]

            assert not char_int in self.combining_classes, "Unexpected: CJK compat variant with a combining class"
            assert not char_int in self.compat_decomp, "Unexpected: CJK compat variant and compatibility decomposition"
            assert len(self.canon_decomp[char_int]) == 1, "Unexpected: CJK compat variant and non-singleton canonical decomposition"
            # If we ever need to handle Hangul here, we'll need to handle it separately.
            assert not (S_BASE <= char_int < S_BASE + S_COUNT)

            cjk_compat_variant_parts = [int(c, 16) for c in variation_sequence.split()]
            for c in cjk_compat_variant_parts:
                assert not c in self.canon_decomp, "Unexpected: CJK compat variant is unnormalized (canon)"
                assert not c in self.compat_decomp, "Unexpected: CJK compat variant is unnormalized (compat)"
            self.cjk_compat_variants_fully_decomp[char_int] = cjk_compat_variant_parts

    def _load_norm_props(self):
        props = collections.defaultdict(list)

        for line in self._fetch("DerivedNormalizationProps.txt").splitlines():
            (prop_data, _, _) = line.partition("#")
            prop_pieces = prop_data.split(";")

            if len(prop_pieces) < 2:
                continue

            assert len(prop_pieces) <= 3
            (low, _, high) = prop_pieces[0].strip().partition("..")

            prop = prop_pieces[1].strip()

            data = None
            if len(prop_pieces) == 3:
                data = prop_pieces[2].strip()
                if prop == "NFKC_CF":
                    # split it into pieces
                    data = data.split(" ")
            props[prop].append((low, high, data))

        return props

    def _load_norm_tests(self):
        tests: list[tuple[str,list]] = []
        part_num = -1
        lines = self._fetch("NormalizationTest.txt").splitlines()
        line: str
        for i, line in enumerate(lines):
            if line.startswith("@"):
                # get the part name; ex: "@Part2 # Canonical Order Test"
                parts = line.split("#", 1)
                # remove the "@Part" to get the number
                part_num = int(parts[0][5:].strip())
                part_name = parts[1].strip()
                tests.append((part_name, []))
                continue
            (test_data, _, comment) = line.partition("#")



            test_pieces = test_data.split(";")

            if len(test_pieces) < 5:
                continue
            # comments go like this: " (Ḋ; Ḋ; D◌̇; Ḋ; D◌̇; ) LATIN CAPITAL LETTER D WITH DOT ABOVE"
            # we want to get the second part of the comment, ex: "LATIN CAPITAL LETTER D WITH DOT ABOVE"
            comment_parts = comment.rsplit(")", 1)
            test_comment = comment_parts[0] + ")"
            test_name = comment_parts[-1].strip()

            source, nfc, nfd, nfkc, nfkd = [[c.strip() for c in p.split()] for p in test_pieces[:5]]

            tests[part_num][1].append(NormalizationTest(test_comment, test_name, source, nfc, nfd, nfkc, nfkd))

        return tests

    def _compute_canonical_comp(self):
        canon_comp = {}
        comp_exclusions = [
            (int(low, 16), int(high or low, 16))
            for low, high, _ in self.norm_props["Full_Composition_Exclusion"]
        ]
        for char_int, decomp in self.canon_decomp.items():
            if any(lo <= char_int <= hi for lo, hi in comp_exclusions):
                continue

            assert len(decomp) == 2
            assert (decomp[0], decomp[1]) not in canon_comp
            canon_comp[(decomp[0], decomp[1])] = char_int

        return canon_comp

    def _compute_fully_decomposed(self):
        """
        Even though the decomposition algorithm is recursive, it is possible
        to precompute the recursion at table generation time with modest
        increase to the table size.  Then, for these precomputed tables, we
        note that 1) compatible decomposition is a subset of canonical
        decomposition and 2) they mostly agree on their intersection.
        Therefore, we don't store entries in the compatible table for
        characters that decompose the same way under canonical decomposition.

            Decomposition table stats:
            Canonical decomp: 2060 chars => 3085 decomposed chars
            Compatible decomp: 3662 chars => 5440 decomposed chars
            Canonical fully decomp: 2060 chars => 3404 decomposed chars
            Compatible fully decomp: 3678 chars => 5599 decomposed chars

        The upshot is that decomposition code is very simple and easy to inline
        at mild code size cost.
        """
        def _decompose(char_int, compatible):
            # 7-bit ASCII never decomposes
            if char_int <= 0x7f:
                yield char_int
                return

            # Assert that we're handling Hangul separately.
            assert not (S_BASE <= char_int < S_BASE + S_COUNT)

            decomp = self.canon_decomp.get(char_int)
            if decomp is not None:
                for decomposed_ch in decomp:
                    for fully_decomposed_ch in _decompose(decomposed_ch, compatible):
                        yield fully_decomposed_ch
                return

            if compatible and char_int in self.compat_decomp:
                for decomposed_ch in self.compat_decomp[char_int]:
                    for fully_decomposed_ch in _decompose(decomposed_ch, compatible):
                        yield fully_decomposed_ch
                return

            yield char_int
            return

        end_codepoint = max(
            max(self.canon_decomp.keys()),
            max(self.compat_decomp.keys()),
        )

        canon_fully_decomp = {}
        compat_fully_decomp = {}

        for char_int in range(0, end_codepoint + 1):
            # Always skip Hangul, since it's more efficient to represent its
            # decomposition programmatically.
            if S_BASE <= char_int < S_BASE + S_COUNT:
                continue

            canon = list(_decompose(char_int, False))
            if not (len(canon) == 1 and canon[0] == char_int):
                canon_fully_decomp[char_int] = canon

            compat = list(_decompose(char_int, True))
            if not (len(compat) == 1 and compat[0] == char_int):
                compat_fully_decomp[char_int] = compat

        # Since canon_fully_decomp is a subset of compat_fully_decomp, we don't
        # need to store their overlap when they agree.  When they don't agree,
        # store the decomposition in the compatibility table since we'll check
        # that first when normalizing to NFKD.
        assert set(canon_fully_decomp) <= set(compat_fully_decomp)

        for ch in set(canon_fully_decomp) & set(compat_fully_decomp):
            if canon_fully_decomp[ch] == compat_fully_decomp[ch]:
                del compat_fully_decomp[ch]

        return canon_fully_decomp, compat_fully_decomp

    def _compute_stream_safe_tables(self):
        """
        To make a text stream-safe with the Stream-Safe Text Process (UAX15-D4),
        we need to be able to know the number of contiguous non-starters *after*
        applying compatibility decomposition to each character.

        We can do this incrementally by computing the number of leading and
        trailing non-starters for each character's compatibility decomposition
        with the following rules:

        1) If a character is not affected by compatibility decomposition, look
           up its canonical combining class to find out if it's a non-starter.
        2) All Hangul characters are starters, even under decomposition.
        3) Otherwise, very few decomposing characters have a nonzero count
           of leading or trailing non-starters, so store these characters
           with their associated counts in a separate table.
        """
        leading_nonstarters = {}
        trailing_nonstarters = {}

        for c in set(self.canon_fully_decomp) | set(self.compat_fully_decomp):
            decomposed = self.compat_fully_decomp.get(c) or self.canon_fully_decomp[c]

            num_leading = 0
            for d in decomposed:
                if d not in self.combining_classes:
                    break
                num_leading += 1

            num_trailing = 0
            for d in reversed(decomposed):
                if d not in self.combining_classes:
                    break
                num_trailing += 1

            if num_leading > 0:
                leading_nonstarters[c] = num_leading
            if num_trailing > 0:
                trailing_nonstarters[c] = num_trailing

        return leading_nonstarters, trailing_nonstarters

hexify = lambda c: '{:04X}'.format(c)

# Test whether `first` and `last` are corresponding "<..., First>" and
# "<..., Last>" markers.
def is_first_and_last(first, last):
    if not first.startswith('<') or not first.endswith(', First>'):
        return False
    if not last.startswith('<') or not last.endswith(', Last>'):
        return False
    return first[1:-8] == last[1:-7]

def gen_mph_data(name, d, kv_type, kv_callback, kv_row_width):
    (salt, keys) = minimal_perfect_hash(d)
    out.write(f"\nconst uint16_t {name.upper()}_SALT[] = {{\n")
    for s_row in batched(salt, 13):
        out.write("   ")
        for s in s_row:
            out.write(f" 0x{s:03X},")
        out.write("\n")
    out.write("};\n\n")
    out.write(f"const size_t {name.upper()}_SALT_SIZE = sizeof({name.upper()}_SALT) / sizeof({name.upper()}_SALT[0]);\n\n")
    out.write(f"const {kv_type} {name.upper()}_KV[] = {{\n")
    for k_row in batched(keys, kv_row_width):
        out.write("   ")
        for k in k_row:
            out.write(f" {kv_callback(k)},")
        out.write("\n")
    out.write("};\n\n")
    out.write(f"const size_t {name.upper()}_KV_SIZE = sizeof({name.upper()}_KV) / sizeof({name.upper()}_KV[0]);\n\n")
    extern_decls = f"""
extern const uint16_t {name.upper()}_SALT[{len(salt)}];
extern const size_t {name.upper()}_SALT_SIZE;
extern const {kv_type} {name.upper()}_KV[{len(keys)}];
extern const size_t {name.upper()}_KV_SIZE;
"""
    return extern_decls

def gen_combining_class(combining_classes, out):
    return gen_mph_data('canonical_combining_class', combining_classes, 'uint32_t',
        lambda k: f"0x{int(combining_classes[k]) | (k << 8):07X}", 8)


def gen_composition_table(canon_comp, out):
    table = {}
    for (c1, c2), c3 in canon_comp.items():
        if c1 < 0x10000 and c2 < 0x10000:
            table[(c1 << 16) | c2] = c3
    (salt, keys) = minimal_perfect_hash(table)
    externs = gen_mph_data('COMPOSITION_TABLE', table, 'CompositionTableKV',
        lambda k: f"{{0x{k:08X}, 0x{table[k]:06X}}}", 1)

    out.write("uint32_t composition_table_astral(uint32_t c1, uint32_t c2) {\n")
    out.write("    switch (c1) {\n")
    last = 0
    for (c1, c2), c3 in sorted(canon_comp.items()):
        if c1 >= 0x10000 and c2 >= 0x10000:
            if c1 != last:
                if last != 0:
                    out.write(f"            break;\n")
                out.write(f"        case 0x{c1:X}:\n")
                last = c1
            out.write("            if (c2 == 0x{:X}) return 0x{:X};\n".format(c2, c3))
    # default
    out.write(f"            break;\n")
    out.write("        default:\n")
    out.write("            break;\n")
    out.write("    }\n")
    out.write("    return 0;\n")
    out.write("}\n")
    return externs

def calc_utf8_len(c):
    if c < 0x80:
        return 1
    if c < 0x800:
        return 2
    if c < 0x10000:
        return 3
    return 4

def gen_decomposition_tables(canon_decomp, compat_decomp, cjk_compat_variants_decomp, NFKC_CF, out):
    tables = [(canon_decomp, 'canonical'), (compat_decomp, 'compatibility'), (cjk_compat_variants_decomp, 'cjk_compat_variants'), (NFKC_CF, 'NFKC_Casefold')]
    externs = []
    for table, name in tables:
        offsets = {}
        offset = 0
        suffix_prefix = "_decomposed" if name != 'NFKC_Casefold' else ""
        suffix = "_DECOMPOSED_CHARS" if name != 'NFKC_Casefold' else "_CHARS"
        out.write("const uint32_t %s%s[] = {\n" % (name.upper(), suffix))
        length = 0
        max_item_len = 0
        max_item_utf8_len = 0
        for k, v in table.items():
            offsets[k] = offset
            offset += len(v)
            this_len = 0
            for c in v:
                out.write("    0x%s,\n" % hexify(c))
                this_len += 1
            length += this_len

        for k in table:
            this_len = len(table[k])
            max_item_len = max(max_item_len, this_len)
            this_utf8_len = 0
            for c in table[k]:
                this_utf8_len += calc_utf8_len(c)
            max_item_utf8_len = max(max_item_utf8_len, this_utf8_len)
        # The largest offset must fit in a u16.
        assert offset < 65536
        out.write("};\n")
        chars_tbl_name = "%s%s" % (name.upper(), suffix)
        out.write("const size_t %s_SIZE = sizeof(%s) / sizeof(%s[0]);\n" % (chars_tbl_name, chars_tbl_name, chars_tbl_name))
        out.write("const size_t %s_MAX_ITEM_LEN = %u;\n" % (chars_tbl_name, max_item_len))
        out.write("const size_t %s_MAX_ITEM_UTF8_LEN = %u;\n" % (chars_tbl_name, max_item_utf8_len))
        externs.append("extern const uint32_t %s[%u];" % (chars_tbl_name, length))
        externs.append("extern const size_t %s_SIZE;" % chars_tbl_name)
        externs.append("extern const size_t %s_MAX_ITEM_LEN;" % chars_tbl_name)
        externs.append("extern const size_t %s_MAX_ITEM_UTF8_LEN;" % chars_tbl_name)
        externs.append(gen_mph_data(name + suffix_prefix, table, "DecomposedKV",
            lambda k: f"{{0x{k:05X}, {{0x{offsets[k]:03X}, 0x{len(table[k]):X}}}}}", 1))
    return "\n".join(externs)

def flatten_prop_table(prop_table):
    flat_table = {}
    for low, high, data in prop_table:
        if high:
            for i in range(int(low, 16), int(high, 16) + 1):
                flat_table[i] = data
        else:
            flat_table[int(low, 16)] = data
    return flat_table

def flatten_nfkc_cf(prop_table):
    flat_table = {}
    for low, high, data in prop_table:
        if high:
            for i in range(int(low, 16), int(high, 16) + 1):
                flat_table[i] = [int(c,16) for c in data if c]
        else:
            flat_table[int(low, 16)] = [int(c,16) for c in data if c]
    return flat_table


def gen_qc_match(prop_table, out):
    out.write("    switch (c) {\n")
    # sort the prop table by "N", "M", then by the low codepoint
    prop_table.sort(key=lambda x: (x[2], int(x[0], 16)))
    # filter out the ones that are not "N" or "M"
    prop_table_no = [x for x in prop_table if x[2] in ('N')]
    prop_table_maybe = [x for x in prop_table if x[2] in ('M')]
    flat_no = flatten_prop_table(prop_table_no)
    for low, data in flat_no.items():

        out.write(r"        case 0x%04X:" % low)
        out.write("\n")
    out.write("            return No;\n")
    for low, data in flatten_prop_table(prop_table_maybe).items():
        out.write(r"        case 0x%04X:" % low)
        out.write("\n")
    out.write("            return Maybe;\n")
    out.write("        default: return Yes;\n")
    out.write("    }\n")
#
def gen_nfc_qc(prop_tables, out):
    # out.write("IsNormalized qc_nfc(uint32_t c) {\n")
    # gen_qc_match(prop_tables['NFC_QC'], out)
    # out.write("}\n")
    return gen_qc_prop_table('NFC_QC', prop_tables['NFC_QC'], out)

def gen_nfkc_qc(prop_tables, out):
    # out.write("IsNormalized qc_nfkc(uint32_t c) {\n")
    # gen_qc_match(prop_tables['NFKC_QC'], out)
    # out.write("}\n")
    return gen_qc_prop_table('NFKC_QC', prop_tables['NFKC_QC'], out)


def gen_nfd_qc(prop_tables, out):
    # out.write("IsNormalized qc_nfd(uint32_t c) {\n")
    # gen_qc_match(prop_tables['NFD_QC'], out)
    # out.write("}\n")
    return gen_qc_prop_table('NFD_QC', prop_tables['NFD_QC'], out)

def gen_nfkd_qc(prop_tables, out):
    # out.write("IsNormalized qc_nfkd(uint32_t c) {\n")
    # gen_qc_match(prop_tables['NFKD_QC'], out)
    # out.write("}\n")
    return gen_qc_prop_table('NFKD_QC', prop_tables['NFKD_QC'], out)

def gen_combining_mark(general_category_mark, out):
    return gen_mph_data('combining_mark', general_category_mark, 'uint32_t',
        lambda k: '0x{:05X}'.format(k), 10)


def prop_table_val_to_int(val):
    if val == 'Y':
        return 0
    if val == 'N':
        return 1
    if val == 'M':
        return 2
    raise ValueError("Unexpected value in prop table: %s" % val)

def gen_qc_prop_table(prop_table_name, prop_table, out):
    flat = flatten_prop_table(prop_table)
    return gen_mph_data(prop_table_name, flat, 'uint32_t',
                        lambda k: f"0x{prop_table_val_to_int(flat[k]) | (k << 8):07X}", 8)


def gen_public_assigned(general_category_public_assigned, out):
    return ""
    # This could be done as a hash but the table is somewhat small.
    # gen_mph_data('public_assigned', general_category_public_assigned, 'uint32_t',
    #     lambda k: f"0x{int(general_category_public_assigned[k][0]) | (k << 8):07X}", 8)
    # out.write("bool is_public_assigned(uint32_t c) {\n")
    # out.write("    match c {\n")
    #
    # start = True
    # for first, last in general_category_public_assigned:
    #     if start:
    #         out.write("        ")
    #         start = False
    #     else:
    #         out.write("\n        | ")
    #     if first == last:
    #         out.write("'\\u{%s}'" % hexify(first))
    #     else:
    #         out.write("'\\u{%s}'..='\\u{%s}'" % (hexify(first), hexify(last)))
    # out.write(" => true,\n")
    #
    # out.write("        _ => false,\n")
    # out.write("    }\n")
    # out.write("}\n")

def gen_stream_safe(leading, trailing, out):
    # This could be done as a hash but the table is very small.
    out.write("\nconst uint32_t stream_safe_leading_nonstarters(uint32_t c) {\n")
    out.write("    switch (c) {\n")
    for char, num_leading in sorted(leading.items()):
        out.write("        case 0x{:X}: return {};\n".format(char, num_leading))
    out.write("        default: return 0;\n")
    out.write("    }\n")
    out.write("}\n")

    return gen_mph_data('trailing_nonstarters', trailing, 'uint32_t',
        lambda k: f"0x{int(trailing[k]) | (k << 8):07X}", 8)


def convert_to_utf8(s):
    # we have to convert the utf-32 string to utf-8
    for c in s:
        # it's a hex string of utf-32, and we are returning a hex utf-8 string representation of the character
        c = int(c, 16)
        if c < 0x80:
            yield "%02x" % c
        elif c < 0x800:
            yield "%02x" % (0xc0 | (c >> 6))
            yield "%02x" % (0x80 | (c & 0x3f))
        elif c < 0x10000:
            yield "%02x" % (0xe0 | (c >> 12))
            yield "%02x" % (0x80 | ((c >> 6) & 0x3f))
            yield "%02x" % (0x80 | (c & 0x3f))
        else:
            yield "%02x" % (0xf0 | (c >> 18))
            yield "%02x" % (0x80 | ((c >> 12) & 0x3f))
            yield "%02x" % (0x80 | ((c >> 6) & 0x3f))
            yield "%02x" % (0x80 | (c & 0x3f))

def convert_to_utf8_ints(s):
    # we have to convert the utf-32 string to utf-8
    for c in s:
        # it's a hex string of utf-32, and we are returning a hex utf-8 string representation of the character
        c = int(c, 16)
        if c < 0x80:
            yield c
        elif c < 0x800:
            yield (0xc0 | (c >> 6))
            yield (0x80 | (c & 0x3f))
        elif c < 0x10000:
            yield (0xe0 | (c >> 12))
            yield (0x80 | ((c >> 6) & 0x3f))
            yield (0x80 | (c & 0x3f))
        else:
            yield (0xf0 | (c >> 18))
            yield (0x80 | ((c >> 12) & 0x3f))
            yield (0x80 | ((c >> 6) & 0x3f))
            yield (0x80 | (c & 0x3f))


def convert_u32_hex_string_to_utf8_real_string(s):
    # turn it into an array of bytes; remember that the values are literal 32-bit values, so `bytes()` won't work
    utf_32_bytes = bytes([c for c in convert_to_utf8_ints(s)])
    thing = utf_32_bytes.decode('utf-8')
    return thing
    # now we have a utf-32 string, we can convert it to utf-8


class thingy:
    def __init__(self, source, src_string, nfkd, nfkd_string):
        self.source = source
        self.src_string = src_string
        self.nfkd = nfkd
        self.nfkd_string = nfkd_string
import shutil

def remove_dir(dir):
    subprocess.check_output(['rm', '-rf', os.path.abspath(dir)])


def test_APFS_hash(tests):
    # make a tmp dir in the current directory
    if os.path.exists("tmp"):
        remove_dir("tmp")
    os.path.exists("tmp") or os.makedirs("tmp")
    os.chdir("tmp")
    # what we are going to do is, for each test, we are going to attempt to make a directory with the string from the source, and the string from the nfkd
    # if we can't make the directory, that's a pass
    # if we can make the directory, that's a fail, add it to the list of fails
    fails:list[thingy] = []
    for test in tests:
        failed = False
        # make the directory
        src_string = convert_u32_hex_string_to_utf8_real_string(test.source)
        nfkd_string = convert_u32_hex_string_to_utf8_real_string(test.nfkd)
        # check if src or nfkd has a slash or any other illegal macos file characters
        if "/" in src_string or "/" in nfkd_string:
            continue
        if ":" in src_string or ":" in nfkd_string:
            continue
        # turn src_string into bytes
        src_temp_dir = src_string
        nfkd_temp_dir = nfkd_string
        # src_string_bytes = src_string.encode('utf-8')
        # nfkd_string_bytes = nfkd_string.encode('utf-8')
        try:
            os.mkdir(src_temp_dir)
        except:
            # we shouldn't have excepted here
            raise Exception(f"Failed to make source directory for {src_string}")
        # make the nfkd directory
        try:
            os.mkdir(nfkd_temp_dir)
            fails.append(thingy(test.source, src_string, test.nfkd, nfkd_string))
            failed = True
        except FileExistsError as e:
            pass
        except:
            # we shouldn't have excepted here
            raise Exception(f"Failed to make nfkd directory for {nfkd_string}")
        # remove the source directory
        try:
            os.rmdir(nfkd_temp_dir)
            if failed:
                os.rmdir(src_temp_dir)
            # list the current directory
            # if there's anything in there, we failed
            if len(os.listdir()) != 0:
                raise Exception("Failed to clean up directories")
        except:
            # back out of the directory
            os.chdir("..")
            # remove the tmp directory
            remove_dir("tmp")
            # remake the tmp directory
            os.makedirs("tmp")
            # move back in
            os.chdir("tmp")
    # chdir back
    os.chdir("..")

    # remove everything in the tmp directory
    for f in os.listdir("tmp"):
        os.rmdir(f)
    # remove the tmp directory
    os.rmdir("tmp")
    return fails

def print_fails(tests):
    fails = test_APFS_hash(tests)
    if len(fails) == 0:
        print("All tests passed")
    else:
        print("The following tests failed:")
        for fail in fails:
            print(f"Source: {fail.src_string} {fail.source}")
            print(f"NFKD: {fail.nfkd_string} {fail.nfkd}")
            print()



def get_c_char(c: str) -> str:
#basic characters:
# | Code unit        | Character                   | Glyph                                                    |
# | ---------------- | --------------------------- | -------------------------------------------------------- |
# | U+0009           | Character tabulation        |                                                          |
# | U+000B           | Line tabulation             |                                                          |
# | U+000C           | Form feed (FF)              |                                                          |
# | U+0020           | Space                       |                                                          |
# | U+0021           | Exclamation mark            | `!`                                                      |
# | U+0022           | Quotation mark              | `"`                                                      |
# | U+0023           | Number sign                 | `#`                                                      |
# | U+0025           | Percent sign                | `%`                                                      |
# | U+0026           | Ampersand                   | `&`                                                      |
# | U+0027           | Apostrophe                  | `'`                                                      |
# | U+0028           | Left parenthesis            | `(`                                                      |
# | U+0029           | Right parenthesis           | `)`                                                      |
# | U+002A           | Asterisk                    | `*`                                                      |
# | U+002B           | Plus sign                   | `+`                                                      |
# | U+002C           | Comma                       | `,`                                                      |
# | U+002D           | Hyphen-minus                | `-`                                                      |
# | U+002E           | Full stop                   | `.`                                                      |
# | U+002F           | Solidus                     | `/`                                                      |
# | U+0030 .. U+0039 | Digit zero .. nine          | `0 1 2 3 4 5 6 7 8 9`                                    |
# | U+003A           | Colon                       | `:`                                                      |
# | U+003B           | Semicolon                   | `;`                                                      |
# | U+003C           | Less-than sign              | `<`                                                      |
# | U+003D           | Equals sign                 | `=`                                                      |
# | U+003E           | Greater-than sign           | `>`                                                      |
# | U+003F           | Question mark               | `?`                                                      |
# | U+0041 .. U+005A | Latin capital letter A .. Z | `A B C D E F G H I J K L M N O P Q R S T U V W X Y Z`    |
# | U+005B           | Left square bracket         | `[`                                                      |
# | U+005C           | Reverse solidus             | `\`                                                      |
# | U+005D           | Right square bracket        | `]`                                                      |
# | U+005E           | Circumflex accent           | `^`                                                      |
# | U+005F           | Low line                    | `_`                                                      |
# | U+0061 .. U+007A | Latin small letter a .. z   | `a b c d e f g h i j k l m n o p q r s t u v w x y z`    |
# | U+007B           | Left curly bracket          | `{`                                                      |
# | U+007C           | Vertical line               | `|`                                                      |
# | U+007D           | Right curly bracket         | `}`                                                      |
# | U+007E           | Tilde                       | `~`                                                      |
    c_int = int(c,16)
    if c_int == 0x0009:
        return "\\t"
    elif c_int == 0x000B:
        return "\\v"
    elif c_int == 0x000C:
        return "\\f"
    elif c_int >= 0x20 and c_int <= 0x7E:
        # we only have to check for the basic characters we have to escape
        if c_int == 0x0022:
            return "\\\""
        if c_int == 0x0027:
            return "\'"
        if c_int == 0x005C:
            return "\\\\"
        return chr(c_int)
    return "\\U%08X" % c_int


    
def gen_tests(tests: list[tuple[str,list]], out):
    # out.write("pub const NORMALIZATION_TESTS: &[NormalizationTest] = &[\n")
    str_literal = lambda s: 'U"%s"' % "".join(get_c_char(c) for c in s)

    out.write("""
#include "normalization_tests.h"

#if defined(__cplusplus) 
    #if __cplusplus >= 201103L
        #define CXX11_CONSTEXPR constexpr
    #else
        #define CXX11_CONSTEXPR const
    #endif
#else
    #define CXX11_CONSTEXPR const
    #ifdef __STDC_VERSION__
        #if __STDC_VERSION__ < 201112L
            #error "Need C11 or greater to run the tests"
        #endif
    #else
        #error "Need C11 or greater to run the tests"
    #endif
#endif

""")
    extern_decls = []
    for i, part in enumerate(tests):

        part_id = "NORMALIZATION_PART_" + str(i) + "_TESTS"
        out.write("// %s\n" % part[0])
        out.write("CXX11_CONSTEXPR NormalizationTest %s[] = {\n" % part_id)
        for test in part[1]:
            out.write("    {\n")
            out.write("        \"%s\",\n" % test.test_name)
            out.write("        %s,\n" % str_literal(test.source))
            out.write("        %s,\n" % str_literal(test.nfc))
            out.write("        %s,\n" % str_literal(test.nfd))
            out.write("        %s,\n" % str_literal(test.nfkc))
            out.write("        %s,\n" % str_literal(test.nfkd))
            out.write("    },\n")
        out.write("};\n\n")
        out.write("CXX11_CONSTEXPR size_t %s_SIZE = sizeof(%s) / sizeof(%s[0]);\n\n" % (part_id, part_id, part_id))
        extern_decls.append(f"extern const NormalizationTest {part_id}[{len(part[1])}];\nextern const size_t {part_id}_SIZE;")
    return "\n".join(extern_decls)

# Guaranteed to be less than n.
def my_hash(x, salt, n):
    # This is hash based on the theory that multiplication is efficient
    mask_32 = 0xffffffff
    y = ((x + salt) * 2654435769) & mask_32
    y ^= (x * 0x31415926) & mask_32
    return (y * n) >> 32

# Compute minimal perfect hash function, d can be either a dict or list of keys.
def minimal_perfect_hash(d):
    n = len(d)
    buckets = dict((h, []) for h in range(n))
    for key in d:
        h = my_hash(key, 0, n)
        buckets[h].append(key)
    bsorted = [(len(buckets[h]), h) for h in range(n)]
    bsorted.sort(reverse = True)
    claimed = [False] * n
    salts = [0] * n
    keys = [0] * n
    for (bucket_size, h) in bsorted:
        # Note: the traditional perfect hashing approach would also special-case
        # bucket_size == 1 here and assign any empty slot, rather than iterating
        # until rehash finds an empty slot. But we're not doing that so we can
        # avoid the branch.
        if bucket_size == 0:
            break
        else:
            for salt in range(1, 32768):
                rehashes = [my_hash(key, salt, n) for key in buckets[h]]
                # Make sure there are no rehash collisions within this bucket.
                if all(not claimed[hash] for hash in rehashes):
                    if len(set(rehashes)) < bucket_size:
                        continue
                    salts[h] = salt
                    for key in buckets[h]:
                        rehash = my_hash(key, salt, n)
                        claimed[rehash] = True
                        keys[rehash] = key
                    break
            if salts[h] == 0:
                print("minimal perfect hashing failed")
                # Note: if this happens (because of unfortunate data), then there are
                # a few things that could be done. First, the hash function could be
                # tweaked. Second, the bucket order could be scrambled (especially the
                # singletons). Right now, the buckets are sorted, which has the advantage
                # of being deterministic.
                #
                # As a more extreme approach, the singleton bucket optimization could be
                # applied (give the direct address for singleton buckets, rather than
                # relying on a rehash). That is definitely the more standard approach in
                # the minimal perfect hashing literature, but in testing the branch was a
                # significant slowdown.
                exit(1)
    return (salts, keys)

if __name__ == '__main__':
    data = UnicodeData()
    # print_fails(data.norm_tests)
    # exit(0)
    # get file name from cli argument
    # if len(sys.argv) != 3:
    #     print("Usage: %s <src_name> <header_name>" % sys.argv[0])
    #     sys.exit(1)
    # filename = sys.argv[1]
    # header = sys.argv[2]
    filename = "src/normtables.c"
    header = "src/normtables.h"
    header_basename = header.split("/")[-1]
    # mkdir -p src and test
    os.path.exists("src") or os.makedirs("src")
    os.path.exists("test") or os.makedirs("test")


    extern_decls = []
    with open(filename, "w", newline = "\n") as out:
        out.write(PREAMBLE)
        out.write(f'#include \"{header_basename}\"\n')
        out.write("\n")
        extern_decls.append(gen_combining_class(data.combining_classes, out))

        extern_decls.append(gen_composition_table(data.canon_comp, out))

        extern_decls.append(gen_decomposition_tables(data.canon_fully_decomp, data.compat_fully_decomp, data.cjk_compat_variants_fully_decomp, flatten_nfkc_cf(data.norm_props['NFKC_CF']), out))

        extern_decls.append(gen_combining_mark(data.general_category_mark, out))

        extern_decls.append(gen_public_assigned(data.general_category_public_assigned, out))

        extern_decls.append(gen_nfc_qc(data.norm_props, out))

        extern_decls.append(gen_nfkc_qc(data.norm_props, out))

        extern_decls.append(gen_nfd_qc(data.norm_props, out))

        extern_decls.append(gen_nfkd_qc(data.norm_props, out))

        extern_decls.append(gen_stream_safe(data.ss_leading, data.ss_trailing, out))
    with open(header, "w", newline = "\n") as out:
        out.write(PREAMBLE)
        out.write(HEADER_CONTENTS)
        out.write("\n".join(extern_decls))

    extern_decls = []
    with open("test/normalization_tests.inl", "w", newline = "\n") as out:
        out.write(PREAMBLE)
        extern_decls.append(gen_tests(data.norm_tests, out))
    with open("test/normalization_tests.h", "w", newline = "\n") as out:
        out.write(PREAMBLE)
        out.write("""
#pragma once
#include <stdint.h>
#include <stddef.h>
#ifdef __cplusplus
#include <uchar.h>
#define _UCSTEST_C32PTR_T const char32_t * 
#define _UCSTEST_C32_T char32_t
#else
#define _UCSTEST_C32PTR_T const uint32_t *
#define _UCSTEST_C32_T uint32_t
#endif

typedef struct {
    const char * test_name;
    _UCSTEST_C32PTR_T source;
    _UCSTEST_C32PTR_T nfc;
    _UCSTEST_C32PTR_T nfd;
    _UCSTEST_C32PTR_T nfkc;
    _UCSTEST_C32PTR_T nfkd;
} NormalizationTest;

""")
        out.write("\n".join(extern_decls))